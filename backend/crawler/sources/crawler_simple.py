# backend/crawler/sources/crawler_simple.py
import requests
import json
from datetime import datetime
import os
import sys

# ----------------------------
# è®©çˆ¬è™«å¯ä»¥æ‰¾åˆ° backend åŒ…
# ----------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = os.path.dirname(os.path.dirname(CURRENT_DIR))
sys.path.append(BACKEND_DIR)

# from src.app.app import create_app
from src.extensions import db
from src.models.restaurant import Restaurant

# # ----------------------------
# # åˆå§‹åŒ– Flask App Context
# app = create_app()
# app.app_context().push()

# ----------------------------
# é«˜å¾· API KEY
# ----------------------------
AMAP_KEY = "d7136435e934f09f79ce84e8ae79f0c2"


def fetch_from_amap():
    """
    è°ƒç”¨é«˜å¾· Web APIï¼Œè·å–å—äº¬â€œç¾é£Ÿâ€ç±»é¤é¦†ã€‚
    """
    url = "https://restapi.amap.com/v3/place/text"
    params = {
        "keywords": "ç¾é£Ÿ",
        "city": "å—äº¬",
        "output": "json",
        "key": AMAP_KEY,
        "page": 1,
        "offset": 5,  # æµ‹è¯•ç”¨ 5 æ¡
    }

    print("æ­£åœ¨è¯·æ±‚é«˜å¾· API ...")
    resp = requests.get(url, params=params)
    data = resp.json()

    # è°ƒè¯•æ—¶å¯ä»¥ä¿ç•™æ‰“å°ï¼Œä¹‹åå¯ä»¥æ³¨é‡Šæ‰
    print(json.dumps(data, indent=2, ensure_ascii=False))
    return data


def parse_poi(poi):
    """
    å°†é«˜å¾·POIç»“æ„è§£ææˆæœ€ç®€ç‰ˆé¤é¦†å­—å…¸ã€‚
    """
    name = poi.get("name")
    address = poi.get("address")

    # ç»çº¬åº¦
    location = poi.get("location", "")
    try:
        lat_str, lng_str = location.split(",")
        lat = float(lat_str)
        lng = float(lng_str)
    except Exception:
        lat, lng = None, None

    # è¯„åˆ†ä¸äººå‡
    biz = poi.get("biz_ext", {})
    rating_raw = biz.get("rating")
    price_raw = biz.get("cost")

    try:
        rating = float(rating_raw) if rating_raw else None
    except Exception:
        rating = None

    try:
        price = int(float(price_raw)) if price_raw else None
    except Exception:
        price = None

    return {
        "name": name,
        "address": address,
        "lat": lat,
        "lng": lng,
        "price": price,
        "rating": rating,
        "source": "amap",
        "last_crawled_time": datetime.now().isoformat(),
    }


def create_or_update_restaurant(data):
    """
    æ ¹æ® name + address å»é‡ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ–°å¢ã€‚
    """
    name = data.get("name")
    address = data.get("address")

    existing = Restaurant.query.filter_by(name=name, address=address).first()

    if existing:
        existing.latitude = data["lat"]
        existing.longitude = data["lng"]
        existing.avg_price = data["price"]
        existing.rating = data["rating"]
        existing.source = data["source"]
        existing.last_crawled_time = datetime.fromisoformat(
            data["last_crawled_time"]
        )
        db.session.commit()
        return "update"

    new_r = Restaurant.from_crawler_data(data)
    db.session.add(new_r)
    db.session.commit()
    return "insert"


def run_crawler_main():
    """
    æä¾›ç»™å®šæ—¶ä»»åŠ¡è°ƒç”¨çš„ä¸»æµç¨‹ï¼š
    1. è¯·æ±‚é«˜å¾·
    2. è§£æ POI
    3. å…¥åº“ï¼ˆinsert/updateï¼‰
    """
    # âœ… åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥ï¼Œé¿å…å¾ªç¯
    from src.app.app import create_app
    
    # åˆ›å»ºåº”ç”¨å’Œä¸Šä¸‹æ–‡
    app = create_app()
    with app.app_context():

        print("ğŸ çˆ¬è™«ä½¿ç”¨çš„æ•°æ®åº“ =", db.engine.url)
        raw = fetch_from_amap()
        pois = raw.get("pois", [])
        print(f"\n[çˆ¬è™«] å…±è·å– {len(pois)} æ¡æ•°æ®\n")

        parsed_list = [parse_poi(p) for p in pois]

        for item in parsed_list:
            result = create_or_update_restaurant(item)
            print(f"[çˆ¬è™«] {item['name']} â†’ {result}")

        print("\n[çˆ¬è™«] æœ¬æ¬¡è¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    # æ‰‹åŠ¨è¿è¡Œè„šæœ¬æ—¶ï¼Œä¹Ÿèµ°åŒä¸€å¥—æµç¨‹
    run_crawler_main()